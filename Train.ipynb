{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Physics Informed Neural Networks (SPINN)\n",
    "\n",
    "A few key notes:\n",
    "\n",
    "(1) The code here is tailored to learn hidden physics within stochastic differential equations of the form:\n",
    "\n",
    "$$ \\frac{dx} = g_1(x,u)dt + \\sqrt{2g_2(x,u)}dw, $$\n",
    "\n",
    "where where $x$ is the system state, $u$ is an exogenous input, $w$ is a Gaussian white noise process, $g_1$ is the drift coefficient, and $g_2$ is the diffusion coefficient. Although the drift and diffusion coefficients represent the underlying physics of the stochastic system, these coefficients may not be known or even measurable in practice. As a result, $g_1$, $g_2$, or individual functions that contribute to $g_1$ and $g_2$ can comprise the the hidden physics of the above equation.\n",
    "\n",
    "(2) This code uses an Euler-Maruyama discretization scheme, which leads to the following description of the state dynamics:\n",
    "\n",
    "$$ x_{k+1} = x_k + g_1(x_k, u_k)\\Delta t + w_k\\sqrt{2g_2(x_k,u_k)\\Delta t}. $$\n",
    "\n",
    "Other discretization schemes can be integrated into the code, however.\n",
    "\n",
    "(3) The code uses unscented transform to propagate stochasticity through the above equations, although other uncertainty propagation methods can be integrated into the code.\n",
    "\n",
    "(4) Example data is provided for two case studies from the original paper (O'Leary et al., 2021) in the file Data.zip. The first case study is a one-state model for directed colloidal self-assembly with an exogenous input and the second is a two-state competitive Lotka-Volterra model with a coexistence equilibrium. In each case, SPINN learns the drift and diffusion coefficients $g_1$ and $g_2$.\n",
    "\n",
    "(5)\"Examples.zip\" contains the results of this exact code run on both examples in (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not write bytecode to maintain clean directories\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "# Import required packages and core code\n",
    "import os\n",
    "import numpy as np\n",
    "import core\n",
    "\n",
    "# Prepare TensorFlow\n",
    "core.prepare_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mean and variance data from path chosen by user\n",
    "# Mean data must be of shape [N, [nx,nu], 1]\n",
    "# Covariance data must be of shape [N, nx, nx]\n",
    "# Data is collected at N+1 discrete time points from repeated stochastic system trajectories\n",
    "# mean_initial/cov_initial consists of mean and covariance estimations at time points 1 to N\n",
    "# mean_final/cov_final consists of means and covariance estimations at time points 2 to N+1\n",
    "# The N total time points can come from one or multiple repeated stochastic system trajectories\n",
    "# In these examples, N = 10^5, which comes from 2000 different 50 time-step trajectories,\n",
    "# each of which start from different initial conditions\n",
    "# Each trajectory is repeated is 10^5 times to estimate the mean and covariance\n",
    "\n",
    "# Colloidal self-assembly system\n",
    "mean_initial = np.load(\"CSA/Data/mean_initial_CSA.npy\")\n",
    "mean_final = np.load(\"CSA/Data/mean_final_CSA.npy\")\n",
    "cov_initial = np.load(\"CSA/Data/cov_initial_CSA.npy\")\n",
    "cov_final = np.load(\"CSA/Data/cov_final_CSA.npy\")\n",
    "\n",
    "## Lotka-Volterra system\n",
    "# mean_initial = np.load(\"LVE/Data/mean_initial_LVE.npy\")\n",
    "# mean_final = np.load(\"LVE/Data/mean_final_LVE.npy\")\n",
    "# cov_initial = np.load(\"LVE/Data/cov_initial_LVE.npy\")\n",
    "# cov_final = np.load(\"LVE/Data/cov_final_LVE.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter relevant system information\n",
    "\n",
    "# Colloidal self-assembly system\n",
    "nx = 1 # state dimension\n",
    "nw = 1 # process noise dimension\n",
    "n = nx + nw # augmented state dimension\n",
    "nu = 1 # exogenous input dimension\n",
    "dt = 1 # time discretization\n",
    "\n",
    "## Lotka-Volterra system\n",
    "# nx = 2 # state dimension\n",
    "# nw = 2 # process noise dimension\n",
    "# n = nx + nw # augmented state dimension\n",
    "# nu = 0 # exogenous input dimension\n",
    "# dt = 0.01 # time discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unscented transform (UT) weights\n",
    "[lam, \n",
    " Wm, \n",
    " Wc] = core.get_weights(nx,\n",
    "                        nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for training neural network that represents the drift coefficient, D1\n",
    "\n",
    "# First choose and create path where data will be saved\n",
    "g1_path = \"CSA/Train_g1/\"\n",
    "# g1_path = \"LVE/Train_g1/\"\n",
    "os.makedirs(g1_path)\n",
    "\n",
    "# Now prepare data\n",
    "[X_train_g1, \n",
    " X_val_g1, \n",
    " X_test_g1, \n",
    " Y_train_g1, \n",
    " Y_val_g1, \n",
    " Y_test_g1, \n",
    " X_mu_g1, \n",
    " X_std_g1, \n",
    " Y_mu_g1, \n",
    " Y_std_g1] = core.g1_train_prep(mean_initial, \n",
    "                                mean_final, \n",
    "                                cov_initial,  \n",
    "                                nx, \n",
    "                                nu, \n",
    "                                nw, \n",
    "                                lam,\n",
    "                                dt,\n",
    "                                g1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter ranges of D1 neural network size parameters\n",
    "n_hidden_layers_g1 = [2, 3] # number of hidden layers\n",
    "n_hidden_nodes_g1 = [10, 20, 50, 100] # number of hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural networks that represent D1\n",
    "core.train_multiple_NNs_g1(X_train_g1, \n",
    "                           X_val_g1, \n",
    "                           X_test_g1, \n",
    "                           Y_train_g1, \n",
    "                           Y_val_g1, \n",
    "                           Y_test_g1, \n",
    "                           n_hidden_layers_g1, \n",
    "                           n_hidden_nodes_g1, \n",
    "                           nx, \n",
    "                           nu, \n",
    "                           Wm, \n",
    "                           g1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose g1 neural network that has lowest test MSE\n",
    "nhl_g1_final = 2 # hidden layers\n",
    "nhn_g1_final = 10 # hidden nodes\n",
    "g1_NN_path = os.path.join(g1_path, str(nhl_g1_final) + \"_HL_\" + str(nhn_g1_final) + \"_Nodes/\")\n",
    "\n",
    "g1_NN = core.load_NN(g1_NN_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training neural network that represents the drift coefficient, D1\n",
    "\n",
    "# First choose and create path where data will be saved\n",
    "g2_path = \"CSA/Train_g2/\"\n",
    "# g2_path = \"LVE/Train_g2/\"\n",
    "os.makedirs(g2_path)\n",
    "\n",
    "# Now prepare data\n",
    "[X_train_g2, \n",
    " X_val_g2, \n",
    " X_test_g2, \n",
    " Y_train_g2, \n",
    " Y_val_g2, \n",
    " Y_test_g2, \n",
    " X_mu_g2, \n",
    " X_std_g2, \n",
    " Y_mu_g2, \n",
    " Y_std_g2] = core.g2_train_prep(g1_NN,\n",
    "                                mean_initial, \n",
    "                                mean_final, \n",
    "                                cov_initial, \n",
    "                                cov_final,\n",
    "                                X_mu_g1, \n",
    "                                X_std_g1, \n",
    "                                Y_mu_g1, \n",
    "                                Y_std_g1,\n",
    "                                nx, \n",
    "                                nu, \n",
    "                                nw, \n",
    "                                lam, \n",
    "                                Wc,\n",
    "                                dt, \n",
    "                                g2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter ranges of g2 neural network size parameters\n",
    "n_hidden_layers_g2 = [2, 3] # number of hidden layers\n",
    "n_hidden_nodes_g2 = [10, 20, 50, 100] # number of hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural networks that represent g2\n",
    "core.train_multiple_NNs_g2(X_train_g2, \n",
    "                           X_val_g2, \n",
    "                           X_test_g2, \n",
    "                           Y_train_g2, \n",
    "                           Y_val_g2,\n",
    "                           Y_test_g2, \n",
    "                           n_hidden_layers_g2, \n",
    "                           n_hidden_nodes_g2, \n",
    "                           nx, \n",
    "                           nu,\n",
    "                           g2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose g2 neural network that has lowest test MSE\n",
    "nhl_g2_final = 2 # hidden layers\n",
    "nhn_g2_final = 20 # hidden nodes\n",
    "g2_NN_path = os.path.join(g2_path, str(nhl_g2_final) + \"_HL_\" + str(nhn_g2_final) + \"_Nodes/\")\n",
    "\n",
    "g2_NN = core.load_NN(g2_NN_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learned hidden physics and true hidden physics\n",
    "\n",
    "# Plot g1\n",
    "\n",
    "# Colloidal self-assembly system\n",
    "core.plot_reconstruction_CSA(g1_NN, X_mu_g1, X_std_g1, Y_mu_g1, Y_std_g1, \"g1\", g1_path)\n",
    "\n",
    "# Lotka-Volterra system\n",
    "# core.plot_reconstruction_LVE(g1_NN, X_mu_g1, X_std_g1, Y_mu_g1, Y_std_g1, \"g1\", g1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot g2\n",
    "\n",
    "# Colloidal self-assembly system\n",
    "core.plot_reconstruction_CSA(g2_NN, X_mu_g2, X_std_g2, Y_mu_g2, Y_std_g2, \"g2\", g2_path)\n",
    "\n",
    "# Lotka-Volterra system\n",
    "# core.plot_reconstruction_LVE(g2_NN, X_mu_g2, X_std_g2, Y_mu_g2, Y_std_g2, \"g2\", g2_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
